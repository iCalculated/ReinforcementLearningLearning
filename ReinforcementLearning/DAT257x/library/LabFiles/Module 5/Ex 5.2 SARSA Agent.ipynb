{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 5: Temporal Difference Learning\n\n### Exercise 5.2: SARSA Agent"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport sys\nfrom collections import defaultdict\n\nif \"../\" not in sys.path:\n    sys.path.append(\"../\") \n    \nfrom lib.envs.simple_rooms import SimpleRoomsEnv\nfrom lib.envs.windy_gridworld import WindyGridworldEnv\nfrom lib.envs.cliff_walking import CliffWalkingEnv\nfrom lib.simulation import Experiment",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Agent(object):  \n        \n    def __init__(self, actions):\n        self.actions = actions\n        self.num_actions = len(actions)\n\n    def act(self, state):\n        raise NotImplementedError",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class SarsaAgent(Agent):\n    \n    def __init__(self, actions, epsilon=0.01, alpha=0.5, gamma=1):\n        super(SarsaAgent, self).__init__(actions)\n        \n        ## Initialize empty dictionary here\n        self.Q = defaultdict(lambda: np.zeros(self.num_actions))\n                        \n        ## In addition, initialize the value of epsilon, alpha and gamma\n        self.epsilon = epsilon\n        self.alpha = alpha\n        self.gamma = gamma \n        \n    def stateToString(self, state):\n        mystring = \"\"\n        if np.isscalar(state):\n            mystring = str(state)\n        else:\n            for digit in state:\n                mystring += str(digit)\n        return mystring    \n    \n    def act(self, state):\n        stateStr = self.stateToString(state)      \n        action = np.random.randint(0, self.num_actions) \n        \n        ## TODO 2\n        ## Implement epsilon greedy policy here\n        A = np.ones(self.num_actions, dtype=float) * self.epsilon / self.num_actions\n        best_actions = np.argwhere(self.Q[stateStr] == np.amax(self.Q[stateStr])).flatten()\n        for i in best_actions:\n            A[i] += (1.0 - self.epsilon)/best_actions.size\n        action = np.random.choice(np.arange(self.num_actions), p = A)\n        #print(best_actions)\n        return action\n\n    def learn(self, state1, action1, reward, state2, action2):\n        state1Str = self.stateToString(state1)\n        state2Str = self.stateToString(state2)\n        \n        ## TODO 3\n        ## Implement the sarsa update here\n        td_target = reward + self.gamma * self.Q[state2Str][action2]\n        td_delta = td_target - self.Q[state1Str][action1]\n        self.Q[state1Str][action1] += self.alpha * td_delta\n        #print(state1Str + \": \" + str(self.Q[state1Str]))\n        \"\"\"\n        SARSA Update\n        Q(s,a) <- Q(s,a) + alpha * (reward + gamma * Q(s',a') - Q(s,a))\n        or\n        Q(s,a) <- Q(s,a) + alpha * (td_target - Q(s,a))\n        or\n        Q(s,a) <- Q(s,a) + alpha * td_delta\n        \"\"\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interactive = True\n%matplotlib nbagg\nenv = SimpleRoomsEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(10, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = SimpleRoomsEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(50, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interactive = True\n%matplotlib nbagg\nenv = CliffWalkingEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(10, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = CliffWalkingEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(100, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "interactive = False\n%matplotlib inline\nenv = WindyGridworldEnv()\nagent = SarsaAgent(range(env.action_space.n))\nexperiment = Experiment(env, agent)\nexperiment.run_sarsa(50, interactive)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}