{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# DAT257x: Reinforcement Learning Explained\n\n## Lab 4: Dynamic Programming\n\n### Exercise 4.4 Value Iteration"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Value Iteration calculates the optimal policy for an MDP, given its full definition.  The full definition of an MDP is the set of states, the set of available actions for each state, the set of rewards, the discount factor, and the state/reward transition function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import test_dp               # required for testing and grading your code\nimport gridworld_mdp as gw   # defines the MDP for a 4x4 gridworld",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Implement the algorithm for Value Iteration**.  Value Iteration calculates the optimal policy for an MDP by iteration of a single step combining both policy evaluation and policy improvement.\n\nA empty function **value_iteration** is provided below; implement the body of the function to correctly calculate the optimal policy for an MDP.  The function defines 5 parameters - a definition of each parameter is given in the comment block for the function.  For sample parameter values, see the calling code in the cell following the function."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\ndef value_iteration(state_count, gamma, theta, get_available_actions, get_transitions):\n    \"\"\"\n    This function computes the optimal value function and policy for the specified MDP, using the Value Iteration algorithm.\n    \n    'state_count' is the total number of states in the MDP. States are represented as 0-relative numbers.\n    \n    'gamma' is the MDP discount factor for rewards.\n    \n    'theta' is the small number threshold to signal convergence of the value function (see Iterative Policy Evaluation algorithm).\n    \n    'get_available_actions' returns a list of the MDP available actions for the specified state parameter.\n    \n    'get_transitions' is the MDP state / reward transiton function.  It accepts two parameters, state and action, and returns\n        a list of tuples, where each tuple is of the form: (next_state, reward, probabiliity).  \n    \"\"\"\n    def lookahead(state, V):\n        poss = 4 * [0]\n        for key, action in enumerate(get_available_actions(state)):\n            for next_state, reward, prob in get_transitions(state,action):\n                poss[key] += prob * (reward + gamma * V[next_state])\n        return poss\n    \n    V = state_count*[0]                # init all state value estimates to 0\n    pi = state_count*[0]\n    \n    # init with a policy with first avail action for each state\n    for s in range(state_count):\n        avail_actions = get_available_actions(s)\n        pi[s] = avail_actions[0]\n        \n    # insert code here to iterate using policy evaluation and policy improvement (see Policy Iteration algorithm)\n    while True:\n        print(pi)\n        policy_stable = True\n        while policy_stable: \n            delta = 0 \n            for state in range(state_count):\n                v = 0\n                action = pi[state]\n                for next_state, reward, probability in get_transitions(state, action):\n                    v +=  probability * (reward + gamma * V[next_state])\n                delta = max(delta, abs(V[state]-v))\n                V[state] = v\n            if delta < theta:\n                break\n\n        for state in range(state_count):\n            old_action = pi[state]\n            pi[state] = get_available_actions(state)[np.argmax(lookahead(state, V))]\n            if pi[state] != old_action:\n                policy_stable = False\n        if policy_stable:\n            return (V, pi)        # return both the final value function and the final policy",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "First, test our function using the MDP defined by gw.* functions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "n_states = gw.get_state_count()\n\n# test our function\nvalues, policy = value_iteration(state_count=n_states, gamma=.9, theta=.001, get_available_actions=gw.get_available_actions, \\\n    get_transitions=gw.get_transitions)\n\nprint(\"Values=\", values)\nprint(\"Policy=\", policy)",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "['up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up']\n['up', 'left', 'up', 'up', 'up', 'left', 'up', 'up', 'up', 'left', 'up', 'down', 'up', 'left', 'right']\n['up', 'left', 'left', 'up', 'up', 'up', 'left', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\nValues= [0.0, -1.0, -1.9, -2.71, -1.0, -1.9, -2.71, -1.9, -1.9, -2.71, -1.9, -1.0, -2.71, -1.9, -1.0]\nPolicy= ['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**Expected output from running above cell:**\n\n`\nValues= [0.0, -1.0, -1.9, -2.71, -1.0, -1.9, -2.71, -1.9, -1.9, -2.71, -1.9, -1.0, -2.71, -1.9, -1.0]\nPolicy= ['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n`"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "Now, test our function using the test_dp helper.  The helper also uses the gw MDP, but with a different gamma value.\nIf our function passes all tests, a passcode will be printed."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# test our function using the test_db helper\ntest_dp.value_iteration_test( value_iteration ) ",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "\nTesting: Value Iteration\n['up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up', 'up']\n['up', 'left', 'up', 'up', 'up', 'left', 'up', 'up', 'up', 'left', 'up', 'down', 'up', 'left', 'right']\n['up', 'left', 'left', 'up', 'up', 'up', 'left', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\n['up', 'left', 'left', 'down', 'up', 'up', 'up', 'down', 'up', 'up', 'down', 'down', 'up', 'right', 'right']\npassed test: return value is tuple\npassed test: length of tuple = 2\npassed test: v is list of length=15\npassed test: values of v elements\npassed test: pi is list of length=15\npassed test: values of pi elements\nPASSED: Value Iteration passcode = 9990-000\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}